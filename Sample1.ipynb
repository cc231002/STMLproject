{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a9a59d17-8f6d-4e54-aa6d-629bd8633af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pyarrow.parquet as pa\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the tokenizer (one-time setup)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "79d59b3f-c002-4a1e-a0a0-411fa0f6bb1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'language_code'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33883 entries, 0 to 33882\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   text           33883 non-null  object\n",
      " 1   language_code  33883 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 529.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Reading the dataset and displaying it\n",
    "data = pd.read_parquet('train-00000-of-00001-a0f92f8fbc6b2308.parquet')\n",
    "print(data.columns)\n",
    "print(data.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "45c9ae81-4ea2-452f-97c8-8087da7d4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text language_code\n",
      "0      Fluoretos binários de metalóides e não-metais ...            pt\n",
      "1      Policja w Birmie postawiła Aung San Suu Kyi dw...            pl\n",
      "2      Ter sistemas de armas nucleares em alerta máxi...            pt\n",
      "3      Взрыв 74+19+16! Дюрэнт и Харден сошли с ума, о...            ru\n",
      "4      Ze had slechts één voorwaarde: dat het boek pa...            nl\n",
      "...                                                  ...           ...\n",
      "33878  Peraza was Miss Venezuela 1976, maar ze trouwd...            nl\n",
      "33879  GIF engraçado: Esta é uma história que acabou ...            pt\n",
      "33880  Sia l'Iran che l'Arabia Saudita rifiutano l'us...            it\n",
      "33881  Scontri al raduno di Morsi al Cairo, decine di...            it\n",
      "33882  Santa Rosa de Lima è un comune del Guatemala, ...            it\n",
      "\n",
      "[33883 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Changing the data from parquet to csv \n",
    "data.to_csv('dataset.csv', index=False)\n",
    "data = pd.read_csv('dataset.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e0d39e71-bdc2-41ee-8866-54cd7eea5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as 'cleaned_dataset.csv'\n",
      "                                                text language_code  \\\n",
      "0  Fluoretos binários de metalóides e não-metais ...            pt   \n",
      "1  Policja w Birmie postawiła Aung San Suu Kyi dw...            pl   \n",
      "2  Ter sistemas de armas nucleares em alerta máxi...            pt   \n",
      "3  Взрыв 74+19+16! Дюрэнт и Харден сошли с ума, о...            ru   \n",
      "4  Ze had slechts één voorwaarde: dat het boek pa...            nl   \n",
      "\n",
      "                                        text_cleaned  \n",
      "0  Fluoretos binários de metalóides e nãometais d...  \n",
      "1  Policja w Birmie postawiła Aung San Suu Kyi dw...  \n",
      "2  Ter sistemas de armas nucleares em alerta máxi...  \n",
      "3  Взрыв Дюрэнт и Харден сошли с ума оба установи...  \n",
      "4  Ze had slechts één voorwaarde dat het boek pas...  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define a cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Handle missing or non-string values\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[0-9]', '', text)  # Remove all numeric digits\n",
    "    text = re.sub(r'[^\\w\\s\\']', '', text)  # Remove unwanted characters (keep words, spaces, and apostrophes)\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()  # Remove leading and trailing whitespace\n",
    "\n",
    "# Step 3: Apply cleaning to all relevant text columns\n",
    "text_columns = ['text']  # Update this with the list of text columns to clean\n",
    "for column in text_columns:\n",
    "    data[f'{column}_cleaned'] = data[column].apply(clean_text)\n",
    "\n",
    "# Step 4: Handle missing values (optional)\n",
    "for column in text_columns:\n",
    "    data.dropna(subset=[f'{column}_cleaned'], inplace=True)  # Drop rows where the cleaned text is missing\n",
    "\n",
    "# Step 5: Save the cleaned data back to CSV\n",
    "data.to_csv('cleaned_dataset.csv', index=False)\n",
    "print(\"Cleaned dataset saved as 'cleaned_dataset.csv'\")\n",
    "\n",
    "# Step 6: Preview the cleaned data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "909a6aba-e3d5-4c04-ba6a-4c4804d95f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved as 'tokenized_dataset.csv'\n",
      "                                                text language_code  \\\n",
      "0  Fluoretos binários de metalóides e não-metais ...            pt   \n",
      "1  Policja w Birmie postawiła Aung San Suu Kyi dw...            pl   \n",
      "2  Ter sistemas de armas nucleares em alerta máxi...            pt   \n",
      "3  Взрыв 74+19+16! Дюрэнт и Харден сошли с ума, о...            ru   \n",
      "4  Ze had slechts één voorwaarde: dat het boek pa...            nl   \n",
      "\n",
      "                                        text_cleaned  \\\n",
      "0  Fluoretos binários de metalóides e nãometais d...   \n",
      "1  Policja w Birmie postawiła Aung San Suu Kyi dw...   \n",
      "2  Ter sistemas de armas nucleares em alerta máxi...   \n",
      "3  Взрыв Дюрэнт и Харден сошли с ума оба установи...   \n",
      "4  Ze had slechts één voorwaarde dat het boek pas...   \n",
      "\n",
      "                                 text_cleaned_tokens  \n",
      "0  [Fluoretos, binários, de, metalóides, e, nãome...  \n",
      "1  [Policja, w, Birmie, postawiła, Aung, San, Suu...  \n",
      "2  [Ter, sistemas, de, armas, nucleares, em, aler...  \n",
      "3  [Взрыв, Дюрэнт, и, Харден, сошли, с, ума, оба,...  \n",
      "4  [Ze, had, slechts, één, voorwaarde, dat, het, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 1: Reload the cleaned dataset\n",
    "data = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Step 2: Define a tokenization function\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):  # Handle missing or non-string values\n",
    "        return []\n",
    "    tokens = word_tokenize(text)  # Tokenize text into words\n",
    "    return tokens\n",
    "\n",
    "# Step 3: Apply tokenization and add as a new column\n",
    "# Replace 'text_column_name_cleaned' with the actual column containing cleaned text\n",
    "cleaned_column = 'text_cleaned'  # Update this to your column name\n",
    "data[f'{cleaned_column}_tokens'] = data[cleaned_column].apply(tokenize_text)\n",
    "\n",
    "# Step 4: Save the tokenized dataset to a new CSV\n",
    "data.to_csv('tokenized_dataset.csv', index=False)\n",
    "print(\"Tokenized dataset saved as 'tokenized_dataset.csv'\")\n",
    "\n",
    "# Step 5: Preview the dataset with the new column\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e93326e2-1187-4682-a260-fc6e6aaab9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33883 entries, 0 to 33882\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   text                 33883 non-null  object\n",
      " 1   language_code        33883 non-null  object\n",
      " 2   text_cleaned         33883 non-null  object\n",
      " 3   text_cleaned_tokens  33883 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.0+ MB\n",
      "None\n",
      "Index(['text', 'language_code', 'text_cleaned', 'text_cleaned_tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.info())\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8b96c-26c7-474f-b4b9-c8cd90193d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
